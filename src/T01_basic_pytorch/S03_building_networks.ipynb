{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 50\n",
    "num_outputs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945702ed",
   "metadata": {},
   "source": [
    "### Manual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4485ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a neural network model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    # First linear layer: transforms input features to 24 features\n",
    "    nn.Linear(num_features, 24),\n",
    "    # Activation function: applies ReLU non-linearity\n",
    "    nn.ReLU(),\n",
    "    # Second linear layer: reduces dimensions to 12 features\n",
    "    nn.Linear(24, 12),\n",
    "    # Activation function: applies ReLU non-linearity\n",
    "    nn.ReLU(),\n",
    "    # Third linear layer: reduces dimensions to 6 features\n",
    "    nn.Linear(12, 6),\n",
    "    # Activation function: applies ReLU non-linearity\n",
    "    nn.ReLU(),\n",
    "    # Output layer: maps 6 features to the desired number of output classes/values\n",
    "    nn.Linear(6, num_outputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "input_size = (batch_size, num_features)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "X = torch.randn(batch_size, num_features)\n",
    "Y = model(X)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb909f0",
   "metadata": {},
   "source": [
    "### Class (recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a123e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        # This line calls the constructor (initializer method) of the parent class, which in this case is nn.Module (from PyTorch).\n",
    "        # When you create a custom neural network by subclassing nn.Module, you must ensure that all the necessary initialization in nn.Module is done properly.\n",
    "        # The super() function allows your class (MyMLP) to inherit and use methods and properties from its parent (nn.Module).\n",
    "        # Without this call, things like parameter registration and model functionality in PyTorch may not work correctly.\n",
    "        super(MyMLP, self).__init__()\n",
    "\n",
    "        # First fully connected layer: input size = num_features, output size = 24\n",
    "        self.fc1 = nn.Linear(num_features, 24)\n",
    "\n",
    "        # Second fully connected layer: input size = 24, output size = 12\n",
    "        self.fc2 = nn.Linear(24, 12)\n",
    "\n",
    "        # Third fully connected layer: input size = 12, output size = 6\n",
    "        self.fc3 = nn.Linear(12, 6)\n",
    "\n",
    "        # Final fully connected layer: input size = 6, output size = num_outputs\n",
    "        self.fc4 = nn.Linear(6, num_outputs)\n",
    "\n",
    "        # ReLU activation function for introducing non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through first layer, then apply ReLU\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass through second layer and apply ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass through third layer and apply ReLU\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass through output layer (no activation)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x  # Return the output (raw predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more concised version\n",
    "class MyMLP_V2(nn.Module):\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 24)\n",
    "        self.fc2 = nn.Linear(24, 12)\n",
    "        self.fc3 = nn.Linear(12, 6)\n",
    "        self.fc4 = nn.Linear(6, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3188513",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "input_size = (batch_size, num_features)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = MyMLP(num_features, num_outputs)\n",
    "batch_size = 10\n",
    "X = torch.randn(batch_size, num_features)\n",
    "Y = model(X)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef76a3",
   "metadata": {},
   "source": [
    "### More complicated example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyComplicatedMLP(nn.Module):\n",
    "    def __init__(self, input_dim=50, hidden_dim=64, output_dim=1):\n",
    "        super(MyComplicatedMLP, self).__init__()\n",
    "\n",
    "        # Linear layers for splitting\n",
    "        self.fc_first = nn.Linear(25, 32)\n",
    "        self.fc_latter = nn.Linear(25, 32)\n",
    "\n",
    "        # Activation functions\n",
    "        self.act_first = nn.ReLU()\n",
    "        self.act_latter = nn.Sigmoid()\n",
    "\n",
    "        # Combine and further processing\n",
    "        self.fc_combined = nn.Linear(64, hidden_dim)\n",
    "        self.act_combined = nn.ReLU()\n",
    "        self.fc_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input: x[:, :25] and x[:, 25:]\n",
    "        x_first = self.act_first(self.fc_first(x[:, :25]))\n",
    "        x_latter = self.act_latter(self.fc_latter(x[:, 25:]))\n",
    "\n",
    "        # Concatenate\n",
    "        x_combined = torch.cat([x_first, x_latter], dim=1)\n",
    "        x = self.act_combined(self.fc_combined(x_combined))\n",
    "        out = self.fc_output(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyComplicatedMLP()\n",
    "input_size = (100, 50)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = MyComplicatedMLP()\n",
    "sample_input = torch.randn(10, 50)  # Batch size of 10\n",
    "output = model(sample_input)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
