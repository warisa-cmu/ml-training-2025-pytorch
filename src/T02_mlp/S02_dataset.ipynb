{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0955e",
   "metadata": {},
   "source": [
    "### What is the PyTorch Dataset class?\n",
    "\n",
    "The PyTorch Dataset class is a base class (called `torch.utils.data.Dataset`) that provides an easy way to work with and load data in PyTorch, especially for training and evaluating machine learning models.\n",
    "\n",
    "Main Purpose\n",
    "It helps you:\n",
    "\n",
    "- Organize your data (images, text, etc.).\n",
    "- Provide a standardized way to access each data point and its label.\n",
    "- Work efficiently with the `DataLoader` for batching, shuffling, and parallel loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15da988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the data and label at index idx\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: 10 samples, each with 2 numbers\n",
    "features = torch.tensor([[i, i + 1] for i in range(10)], dtype=torch.float32)\n",
    "\n",
    "# Labels: 10 numbers (e.g., sum of the features for demonstration)\n",
    "labels = torch.tensor([i + (i + 1) for i in range(10)], dtype=torch.float32)\n",
    "\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom dataset with features and labels\n",
    "dataset = MyCustomDataset(features, labels)\n",
    "\n",
    "# Print the total number of samples in the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# Print the first data sample and its label (index 0)\n",
    "print(dataset[0])\n",
    "\n",
    "# Print a slice of the dataset (from index 0 up to, but not including, index 3)\n",
    "print(dataset[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23651f2",
   "metadata": {},
   "source": [
    "### What is PyTorch DataLoader?\n",
    "\n",
    "The PyTorch DataLoader is a convenient tool that helps you load data efficiently during model training or evaluation. It works together with a Dataset to provide batches of data, handles shuffling, and can load data in parallel to speed up training.\n",
    "\n",
    "Key Features\n",
    "- Batches your data automatically (e.g., batch size of 32 means you get 32 samples at a time).\n",
    "- Shuffles your data if needed, which helps prevent the model from learning the order.\n",
    "- Loads data in parallel with multiple worker processes for speed (especially useful for large datasets).\n",
    "- Iterates easily through your dataset, so you don’t have to write custom loops for slicing or batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75631e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader to load data from the dataset in batches of size 3, shuffling the data each epoch\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Loop through the DataLoader, which yields batches of features and labels\n",
    "for batch_features, batch_labels in dataloader:\n",
    "    print(\"Batch features:\", batch_features)\n",
    "    print(\"Batch labels:\", batch_labels)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7eff2f",
   "metadata": {},
   "source": [
    "### Let's write a custom dataset\n",
    "\n",
    "This dataset receives numpy arrays and returns PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetPT(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_out = torch.from_numpy(self.X[idx, :]).float()\n",
    "        Y_out = torch.from_numpy(self.Y[idx, :]).float()\n",
    "        return X_out, Y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19640806",
   "metadata": {},
   "source": [
    "### Custom DataHandler\n",
    "\n",
    "- Stores Raw Data and Scalers\n",
    "    - Takes in raw feature data (_X), target values (_Y), and scaling objects (scalerX, scalerY)—these could be, for example, StandardScaler or MinMaxScaler from scikit-learn.\n",
    "Stores both the data and the scalers as attributes.\n",
    "- Splits and Scales Data\n",
    "    - `split_and_scale(test_size, random_state, val_size=0)`\n",
    "        - Splits the data into training, test, and (optionally) validation sets.\n",
    "        - Scales each set using the provided scalers.\n",
    "        - The scalers are fit on the training data and then transform all parts.\n",
    "    - Handles the validation split correctly, so you get exactly the fractions you want (e.g., 80% train, 10% val, 10% test).\n",
    "- Prepares PyTorch Datasets\n",
    "    - Methods get_train(), get_val(), and get_test() wrap each data split into a DatasetPT—your custom Dataset class from earlier. This makes the data easy to feed into PyTorch DataLoader objects.\n",
    "    - If validation data isn’t present, get_val() will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853efa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandlerPT(Dataset):\n",
    "    def __init__(self, _X, _Y, scalerX, scalerY):\n",
    "        self._X = _X\n",
    "        self._Y = _Y\n",
    "        self.scalerX = scalerX\n",
    "        self.scalerY = scalerY\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.X_val = None\n",
    "        self.Y_train = None\n",
    "        self.Y_val = None\n",
    "        self.Y_test = None\n",
    "\n",
    "    def split_and_scale(self, test_size, random_state, val_size=0):\n",
    "        _X_train, _X_test, _Y_train, _Y_test = train_test_split(\n",
    "            self._X, self._Y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        self.scalerX.fit(_X_train)\n",
    "        self.scalerY.fit(_Y_train)\n",
    "\n",
    "        if val_size > 0:\n",
    "            _X_train, _X_val, _Y_train, _Y_val = train_test_split(\n",
    "                _X_train,\n",
    "                _Y_train,\n",
    "                # For example, if you want 80% train, 10% validation, and 10% test:\n",
    "                # First, split off the test set (10%):\n",
    "                # Next, split the remaining 90% into train and validation.\n",
    "                # Since you want 80% train and 10% validation overall, the validation set should be 10/90 = 0.111 of the remaining data.\n",
    "                test_size=val_size / (1 - test_size),\n",
    "                random_state=random_state + 100,  # Just make random_state different.\n",
    "            )\n",
    "            self.X_val = self.scalerX.transform(_X_val)\n",
    "            self.Y_val = self.scalerY.transform(_Y_val)\n",
    "\n",
    "        self.X_train = self.scalerX.transform(_X_train)\n",
    "        self.X_test = self.scalerX.transform(_X_test)\n",
    "\n",
    "        self.Y_train = self.scalerY.transform(_Y_train)\n",
    "        self.Y_test = self.scalerY.transform(_Y_test)\n",
    "\n",
    "    # This part is different from SKLearn version\n",
    "    def get_train(self):\n",
    "        return DatasetPT(X=self.X_train, Y=self.Y_train)\n",
    "\n",
    "    def get_val(self):\n",
    "        if self.X_val is None:\n",
    "            raise Exception(\"No validation data\")\n",
    "        return DatasetPT(X=self.X_val, Y=self.Y_val)\n",
    "\n",
    "    def get_test(self):\n",
    "        return DatasetPT(X=self.X_test, Y=self.Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use Pytorch Dataset and DataLoader classes\n",
    "X = np.random.rand(100, 10)  # 100 samples, 10 features each\n",
    "Y = np.random.rand(100, 2)  # 100 targets\n",
    "\n",
    "ds = DatasetPT(X, Y)\n",
    "loader = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "\n",
    "for X_batch, Y_batch in loader:\n",
    "    print(X_batch.shape, Y_batch.shape, Y_batch[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f19d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use DataHandlerPT\n",
    "np.random.seed(0)\n",
    "_X = np.random.rand(100, 10)  # 100 samples, 10 features each\n",
    "_Y = np.random.rand(100, 2)  # 100 targets\n",
    "\n",
    "data_handler = DataHandlerPT(_X, _Y, scalerX=StandardScaler(), scalerY=StandardScaler())\n",
    "\n",
    "# Split with validation\n",
    "data_handler.split_and_scale(test_size=0.1, val_size=0.1, random_state=0)\n",
    "\n",
    "ds_train = data_handler.get_train()\n",
    "ds_val = data_handler.get_val()\n",
    "ds_test = data_handler.get_test()\n",
    "\n",
    "for ds in [ds_train, ds_val, ds_test]:\n",
    "    X, Y = ds[:]\n",
    "    print(X.shape, Y.shape, Y[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e407d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split without validation\n",
    "data_handler.split_and_scale(test_size=0.1, random_state=0)\n",
    "\n",
    "ds_train = data_handler.get_train()\n",
    "ds_test = data_handler.get_test()\n",
    "\n",
    "for ds in [ds_train, ds_test]:\n",
    "    X, Y = ds[:]\n",
    "    print(X.shape, Y.shape, Y[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data.xlsx\", index_col=\"exp\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = df.iloc[:, :-3].values\n",
    "_Y = df.iloc[:, -3:].values\n",
    "print(_X.shape)\n",
    "print(_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandlerPT(\n",
    "    _X=_X, _Y=_Y, scalerX=StandardScaler(), scalerY=StandardScaler()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.split_and_scale(test_size=0.2, val_size=0.1, random_state=0)\n",
    "ds_train = data_handler.get_train()\n",
    "ds_val = data_handler.get_val()\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "loader_val = DataLoader(ds_val, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Train\")\n",
    "for X_batch, Y_batch in loader_train:\n",
    "    print(X_batch.shape, Y_batch.shape, Y_batch[0, :])\n",
    "\n",
    "print(\"Val\")\n",
    "for X_batch, Y_batch in loader_val:\n",
    "    print(X_batch.shape, Y_batch.shape, Y_batch[0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
