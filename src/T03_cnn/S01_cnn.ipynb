{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd55ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pt_runner.cnn import CheckpointHandler, DataHandlerPT, EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc5cecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New run\n",
    "NEW_RUN = True\n",
    "DT_REF = None\n",
    "\n",
    "# Resuming\n",
    "# NEW_RUN = False\n",
    "# DT_REF = \"2025-05-25_08-58\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "015c23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac2a5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_small.pickle\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e69a838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1, 28, 28)\n",
      "float64\n",
      "(2000, 1)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "_X = data[\"_X\"].astype(float)\n",
    "_Y = data[\"_Y\"].astype(float)\n",
    "print(_X.shape)\n",
    "print(_X.dtype)\n",
    "print(_Y.shape)\n",
    "print(_Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad4548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandlerPT(_X=_X, _Y=_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f511d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9465e-02, -1.9132e-01, -1.4498e-01,  9.3011e-02,  2.1806e-01,\n",
       "         -1.6085e-01, -4.5651e-02, -3.1434e-02, -1.7815e-01, -8.9619e-02],\n",
       "        [-7.1117e-02, -1.6171e-01, -1.4174e-01,  1.4484e-01,  2.8498e-01,\n",
       "         -1.5732e-01,  1.3213e-02, -2.0569e-04, -1.6518e-01, -6.5073e-02],\n",
       "        [-3.4264e-02, -9.3897e-02, -1.1449e-01,  4.3201e-02,  1.9634e-01,\n",
       "         -1.2571e-01, -2.3975e-02, -5.9899e-02, -1.7136e-01, -4.4362e-02],\n",
       "        [-6.9071e-03, -1.4762e-01, -7.4263e-02,  1.0062e-01,  2.5548e-01,\n",
       "         -1.3183e-01,  5.9588e-03, -2.7009e-02, -1.1527e-01, -1.1156e-01],\n",
       "        [-8.1074e-03, -1.8677e-01, -3.9886e-02,  1.1900e-01,  2.7171e-01,\n",
       "         -1.4307e-01, -2.6917e-04,  6.2861e-03, -1.4744e-01, -4.7850e-02],\n",
       "        [-1.4945e-01, -7.0126e-02, -8.9559e-02,  5.0698e-02,  2.8521e-01,\n",
       "         -1.3894e-01, -2.4447e-03, -5.5963e-02, -1.4848e-01, -1.2180e-01],\n",
       "        [-7.8281e-02, -1.7695e-01, -1.0124e-01,  9.2421e-02,  2.3565e-01,\n",
       "         -1.4593e-01, -1.4129e-02, -3.0522e-02, -1.7757e-01, -9.3499e-02],\n",
       "        [-2.2395e-02, -1.4662e-01, -9.2692e-02,  1.0520e-01,  2.6520e-01,\n",
       "         -7.5211e-02, -1.3627e-02, -3.5726e-02, -1.0749e-01, -9.4136e-02],\n",
       "        [-7.9572e-02, -1.5912e-01, -8.6354e-02,  9.3110e-02,  2.2286e-01,\n",
       "         -1.2549e-01,  1.3078e-02, -6.6739e-02, -1.7057e-01, -9.1758e-02],\n",
       "        [-4.9795e-02, -1.0366e-01, -7.5602e-02,  1.1344e-01,  2.3195e-01,\n",
       "         -1.0928e-01, -2.6351e-02, -4.2783e-02, -1.0236e-01, -1.1507e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer: input channels=3 (e.g., RGB), output channels=16, kernel size=3\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, num_classes)  # Adjust input size depending on image size!\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 1st Conv + Activation + Pooling\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.max_pool(X)\n",
    "        # 2nd Conv + Activation + Pooling\n",
    "        X = self.conv2(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.max_pool(X)\n",
    "\n",
    "        X = self.adaptive_pool(X)\n",
    "        # Flatten the output for the fully connected layer\n",
    "        X = X.view(X.shape[0], -1)\n",
    "        # Fully connected output\n",
    "        X = self.fc1(X)\n",
    "        return X\n",
    "\n",
    "model = SimpleCNN(num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Test\n",
    "X = torch.randn(10,1, 28,28)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "302994cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleCNN                                [100, 10]                 --\n",
       "├─Conv2d: 1-1                            [100, 16, 32, 32]         160\n",
       "├─ReLU: 1-2                              [100, 16, 32, 32]         --\n",
       "├─MaxPool2d: 1-3                         [100, 16, 16, 16]         --\n",
       "├─Conv2d: 1-4                            [100, 32, 16, 16]         4,640\n",
       "├─ReLU: 1-5                              [100, 32, 16, 16]         --\n",
       "├─MaxPool2d: 1-6                         [100, 32, 8, 8]           --\n",
       "├─AdaptiveAvgPool2d: 1-7                 [100, 32, 4, 4]           --\n",
       "├─Linear: 1-8                            [100, 10]                 5,130\n",
       "==========================================================================================\n",
       "Total params: 9,930\n",
       "Trainable params: 9,930\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 135.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.41\n",
       "Forward/backward pass size (MB): 19.67\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 20.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "input_size = (100, 1, 32, 32) # (batch_size, channels, height, width)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22a25d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ff4562635342b88dd87e3da823802a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m optimizer.zero_grad()\n\u001b[32m     49\u001b[39m Y_pred = model(X_batch)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     52\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Coding\\class\\ml-training-2025-pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Coding\\class\\ml-training-2025-pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Coding\\class\\ml-training-2025-pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Coding\\class\\ml-training-2025-pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000  # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "validation_interval = 10  # Evaluate every 100 epochs\n",
    "log_name = \"M1\"\n",
    "random_state = 0  # Split data\n",
    "\n",
    "# Save/load\n",
    "cph = CheckpointHandler()\n",
    "cph.make_dir(\"./checkpoints\")\n",
    "if NEW_RUN:\n",
    "    dt = cph.get_dt()\n",
    "    log_dir = f\"runs/{dt}\"\n",
    "    save_path = f\"./checkpoints/{dt}.pth\"\n",
    "    epoch_start = 0\n",
    "else:\n",
    "    log_dir = f\"runs/{DT_REF}\"\n",
    "    load_path = f\"./checkpoints/{DT_REF}.pth\"\n",
    "    save_path = load_path\n",
    "    model, optimizer, epoch, val_loss = cph.load(\n",
    "        load_path=load_path, model=model, optimizer=optimizer\n",
    "    )\n",
    "    epoch_start = epoch\n",
    "    print(f\"Resuming from epoch: {epoch}\")\n",
    "\n",
    "epoch_end = epoch_start + n_epochs\n",
    "\n",
    "# Initialize Components\n",
    "early_stopper = EarlyStopper(patience=10)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Data\n",
    "data_handler.split_and_scale(test_size=0.2, val_size=0.1, random_state=RANDOM_STATE)\n",
    "ds_train = data_handler.get_train()\n",
    "ds_test = data_handler.get_test()\n",
    "ds_val = data_handler.get_val()\n",
    "loader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Main loop\n",
    "for epoch in tqdm(\n",
    "    range(epoch_start, epoch_end), initial=epoch_start, desc=\"Epoch\", total=n_epochs\n",
    "):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    for X_batch, Y_batch in loader_train:\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch)\n",
    "        loss = loss_fn(Y_pred, Y_batch)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Multiplies the average loss per sample by the number of\n",
    "        # samples in the batch to get the total loss for this batch.\n",
    "        epoch_train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(loader_train.dataset)\n",
    "\n",
    "    # Validation Phase\n",
    "    if epoch % validation_interval == 0 or epoch == epoch_start:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in loader_val:\n",
    "                Y_pred = model(X_val)\n",
    "                val_loss += loss_fn(Y_pred, Y_val).item() * X_val.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(loader_val.dataset)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early Stopping and Checkpoint\n",
    "        es = early_stopper(avg_val_loss)\n",
    "        if es[\"best_loss\"]:\n",
    "            cph.save(\n",
    "                save_path=save_path,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                val_loss=avg_val_loss,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            print(\"Save model @ epoch:\", epoch)\n",
    "        if es[\"early_stop\"]:\n",
    "            print(\"Stopped at epoch:\", epoch)\n",
    "            break\n",
    "\n",
    "        writer.add_scalars(\n",
    "            log_name, {\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss}, epoch\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f56b314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d127086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05614dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
